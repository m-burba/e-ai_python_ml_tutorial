
%==============================================================================
%
%==============================================================================
\section{Using GPUs for Training}
\subsection{Checking GPU Availability and Installing Required Packages}
To use GPUs in Python, we need to install and verify the necessary packages such as PyTorch.

\begin{codeonly}{Checking GPU Availability}
import torch

# Check if CUDA (NVIDIA GPU) is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Check how many GPUs are available
print(f"Number of GPUs available: {torch.cuda.device_count()}")

# Get GPU name if available
if torch.cuda.is_available():
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
\end{codeonly}

\subsection{Exploring Tensors on the GPU}
Once we load tensors onto the GPU, we can verify their placement and explore GPU memory usage.

\begin{codeonly}{Working with Tensors on GPU}
# Create a tensor and move it to the GPU
tensor_cpu = torch.randn(5, 5)
tensor_gpu = tensor_cpu.to("cuda") if torch.cuda.is_available() else tensor_cpu

print("Tensor on GPU:", tensor_gpu)
print("Tensor Device:", tensor_gpu.device)

# Check GPU memory usage if available
if torch.cuda.is_available():
    print("Allocated GPU Memory:", torch.cuda.memory_allocated() / 1e6, "MB")
    print("Cached GPU Memory:", torch.cuda.memory_reserved() / 1e6, "MB")
\end{codeonly}\end{document}


\subsection{Training a Neural Network with GPU Acceleration}
In this example, we train a deep neural network on a synthetic dataset, comparing training times on CPU and GPU.

\begin{codeonly}{Training a Neural Network on GPU}
import torch.nn as nn
import torch.optim as optim
import time
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic training data
def true_function(x):
    return 2.0 * torch.sin(3.0 * x) + 0.5 * torch.cos(5.0 * x) + 0.2 * x**2 - 0.3 * x + 1.0

# Create dataset
x_train = torch.linspace(-5, 5, 500).view(-1, 1)
y_train = true_function(x_train)

# Define a deeper neural network
class ComplexNN(nn.Module):
    def __init__(self):
        super(ComplexNN, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(1, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    def forward(self, x):
        return self.fc(x)

# Training function
def train_model(device, epochs=2000):
    model = ComplexNN().to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    x_train_device = x_train.to(device)
    y_train_device = y_train.to(device)
    
    start_time = time.time()
    for epoch in range(epochs):
        optimizer.zero_grad()
        y_pred = model(x_train_device)
        loss = criterion(y_pred, y_train_device)
        loss.backward()
        optimizer.step()
        
        if epoch % 200 == 0:
            print(f"Epoch {epoch}: Loss = {loss.item():.6f}")
    
    elapsed_time = time.time() - start_time
    print(f"Training completed in {elapsed_time:.3f} seconds on {device}")
    return model, elapsed_time

# Train on CPU
print("\nTraining on CPU...")
cpu_model, cpu_time = train_model(torch.device("cpu"))

# Train on GPU (if available)
if torch.cuda.is_available():
    print("\nTraining on GPU...")
    gpu_model, gpu_time = train_model(torch.device("cuda"))
else:
    gpu_time = None

# Compare results
print("\n--- Training Time Comparison ---")
print(f"CPU Training Time: {cpu_time:.3f} seconds")
if gpu_time:
    print(f"GPU Training Time: {gpu_time:.3f} seconds")
    print(f"Speedup Factor: {cpu_time / gpu_time:.2f}x")
\end{codeonly}

\subsection{Comparing Model Predictions on CPU and GPU}
After training, we compare the predictions from both CPU and GPU models.

\begin{codeonly}{Visualizing Predictions}
x_test = torch.linspace(-5, 5, 500).view(-1, 1)
y_true = true_function(x_test)

cpu_model.eval()
y_cpu_pred = cpu_model(x_test).detach()

if torch.cuda.is_available():
    gpu_model.eval()
    y_gpu_pred = gpu_model(x_test.to("cuda")).cpu().detach()

plt.figure(figsize=(8, 5))
plt.plot(x_test, y_true, label="True Function", linestyle="dashed", color="black")
plt.plot(x_test, y_cpu_pred, label="CPU Prediction", color="red")
if gpu_time:
    plt.plot(x_test, y_gpu_pred, label="GPU Prediction", color="blue")
plt.legend()
plt.title("CPU vs GPU Model Prediction - Complex NN")
plt.xlabel("x")
plt.ylabel("y")
plt.show()
\end{codeonly}

This chapter provides a comprehensive guide on utilizing GPUs for deep learning applications, from checking GPU availability to training and comparing model performance on different devices.
